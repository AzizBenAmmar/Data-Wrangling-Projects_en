{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.13"
    },
    "colab": {
      "name": "Data Cleaning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpc3IBjCrrqr",
        "colab_type": "text"
      },
      "source": [
        "# Correcting Validity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSB673V-rrqx",
        "colab_type": "text"
      },
      "source": [
        "Your task is to check the \"productionStartYear\" of the DBPedia autos datafile for valid values.\n",
        "The following things should be done:\n",
        "- check if the field \"productionStartYear\" contains a year\n",
        "- check if the year is in range 1886-2014\n",
        "- convert the value of the field to be just a year (not full datetime)\n",
        "- the rest of the fields and values should stay the same\n",
        "- if the value of the field is a valid year in the range as described above,\n",
        "  write that line to the output_good file\n",
        "- if the value of the field is not a valid year as described above, \n",
        "  write that line to the output_bad file\n",
        "- discard rows (neither write to good nor bad) if the URI is not from dbpedia.org\n",
        "- you should use the provided way of reading and writing data (DictReader and DictWriter)\n",
        "  They will take care of dealing with the header.\n",
        "\n",
        "You can write helper functions for checking the data and writing the files, but we will call only the \n",
        "'process_file' with 3 arguments (inputfile, output_good, output_bad)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEQ0BdoJxwmB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from dateutil.parser import parse\n",
        "\n",
        "def is_date(string, fuzzy=False):\n",
        "    try: \n",
        "        parse(string, fuzzy=fuzzy)\n",
        "        return True\n",
        "\n",
        "    except ValueError:\n",
        "        return False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwcSCWC9rrq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import csv\n",
        "import pprint\n",
        "import xlrd\n",
        "\n",
        "INPUT_FILE = 'autos.csv'\n",
        "OUTPUT_GOOD = 'autos-valid.csv'\n",
        "OUTPUT_BAD = 'FIXME-autos.csv'\n",
        "\n",
        "def process_file(input_file, output_good, output_bad):\n",
        "    good_data = []\n",
        "    bad_data = []\n",
        "    with open(input_file, \"r\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        header = reader.fieldnames\n",
        "\n",
        "        #COMPLETE THIS FUNCTION\n",
        "        for line in reader:\n",
        "          bad_line = False\n",
        "          if (line['productionStartYear'] == 'NULL') & (not bad_line):\n",
        "            bad_data.append(line)\n",
        "            bad_line = True\n",
        "          if (not is_date(line['productionStartYear'])) & (not bad_line):\n",
        "            bad_data.append(line)\n",
        "            bad_line = True\n",
        "          if (not bad_line):\n",
        "            year = line['productionStartYear'].split('-')[0]\n",
        "\n",
        "            if int(year)>=1886 and int(year)<2014:\n",
        "              line['productionStartYear'] = year\n",
        "              good_data.append(line)\n",
        "            else:\n",
        "              bad_data.append(line)\n",
        "\n",
        "\n",
        "    # This is just an example on how you can use csv.DictWriter\n",
        "    # Remember that you have to output 2 files\n",
        "    with open(output_good, \"w\") as g:\n",
        "        writer = csv.DictWriter(g, delimiter=\",\", fieldnames= header)\n",
        "        writer.writeheader()\n",
        "        for row in good_data:\n",
        "            writer.writerow(row)\n",
        "    with open(output_bad, \"w\") as g:\n",
        "        writer = csv.DictWriter(g, delimiter=\",\", fieldnames= header)\n",
        "        writer.writeheader()\n",
        "        for row in bad_data:\n",
        "            writer.writerow(row)\n",
        "\n",
        "\n",
        "def test():\n",
        "\n",
        "    process_file(INPUT_FILE, OUTPUT_GOOD, OUTPUT_BAD)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhlsONlJrrrA",
        "colab_type": "text"
      },
      "source": [
        "# Profiling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDiPQTzErrrC",
        "colab_type": "text"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "#-*- coding: utf-8 -*-\n",
        "\n",
        "In this problem set you work with cities infobox data, audit it, come up with a\n",
        "cleaning idea and then clean it up. In the first exercise we want you to audit\n",
        "the datatypes that can be found in some particular fields in the dataset.\n",
        "The possible types of values can be:\n",
        "- NoneType if the value is a string \"NULL\" or an empty string \"\"\n",
        "- list, if the value starts with \"{\"\n",
        "- int, if the value can be cast to int\n",
        "- float, if the value can be cast to float, but CANNOT be cast to int.\n",
        "   For example, '3.23e+07' should be considered a float because it can be cast\n",
        "   as float but int('3.23e+07') will throw a ValueError\n",
        "- 'str', for all other values\n",
        "\n",
        "The audit_file function should return a dictionary containing fieldnames and a \n",
        "SET of the types that can be found in the field. e.g.\n",
        "{\"field1\": set([type(float()), type(int()), type(str())]),\n",
        " \"field2\": set([type(str())]),\n",
        "  ....\n",
        "}\n",
        "The type() function returns a type object describing the argument given to the \n",
        "function. You can also use examples of objects to create type objects, e.g.\n",
        "type(1.1) for a float: see the test function below for examples.\n",
        "\n",
        "Note that the first three rows (after the header row) in the cities.csv file\n",
        "are not actual data points. The contents of these rows should not be included\n",
        "when processing data types. Be sure to include functionality in your code to\n",
        "skip over or detect these rows.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUC_gg5srrrF",
        "colab_type": "code",
        "outputId": "6d2d5a17-fd6f-4f5b-b99e-70ecbf50a9ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "import codecs\n",
        "import csv\n",
        "import json\n",
        "import pprint\n",
        "\n",
        "CITIES = 'cities.csv'\n",
        "\n",
        "FIELDS = [\"name\", \"timeZone_label\", \"utcOffset\", \"homepage\", \"governmentType_label\",\n",
        "          \"isPartOf_label\", \"areaCode\", \"populationTotal\", \"elevation\",\n",
        "          \"maximumElevation\", \"minimumElevation\", \"populationDensity\",\n",
        "          \"wgs84_pos#lat\", \"wgs84_pos#long\", \"areaLand\", \"areaMetro\", \"areaUrban\"]\n",
        "\n",
        "def audit_file(filename, fields):\n",
        "    fieldtypes = {}\n",
        "\n",
        "    # init types\n",
        "    for field in FIELDS:\n",
        "      fieldtypes[field] = set()\n",
        "    \n",
        "    # fill the types\n",
        "    with open(filename,\"rb\") as f:\n",
        "        r = csv.DictReader(f)\n",
        "        for i in range(3):\n",
        "            r.next()\n",
        "\n",
        "        for line in r:\n",
        "          for field in FIELDS:\n",
        "            if line[field]=='NULL' or line[field]==' ':\n",
        "              fieldtypes[field].add(type(None))\n",
        "              continue\n",
        "            if line[field].startswith(\"{\"):\n",
        "              fieldtypes[field].add(type(list()))\n",
        "              continue\n",
        "            try:\n",
        "              int(line[field])\n",
        "              fieldtypes[field].add(type(int()))\n",
        "            except ValueError:\n",
        "              try:\n",
        "                float(line[field])\n",
        "                fieldtypes[field].add(type(float()))\n",
        "              except ValueError:\n",
        "                fieldtypes[field].add(type(str()))\n",
        "    \n",
        "\n",
        "    return fieldtypes\n",
        "\n",
        "\n",
        "def test():\n",
        "    fieldtypes = audit_file(CITIES, FIELDS)\n",
        "\n",
        "    pprint.pprint(fieldtypes)\n",
        "\n",
        "    assert fieldtypes[\"areaLand\"] == set([type(float()),type(list()),type(None)]) \n",
        "    \n",
        "    \n",
        "if __name__ == \"__main__\":\n",
        "    test()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'areaCode': set([<type 'int'>,\n",
            "                  <type 'NoneType'>,\n",
            "                  <type 'str'>,\n",
            "                  <type 'list'>]),\n",
            " 'areaLand': set([<type 'float'>, <type 'NoneType'>, <type 'list'>]),\n",
            " 'areaMetro': set([<type 'float'>, <type 'NoneType'>, <type 'list'>]),\n",
            " 'areaUrban': set([<type 'float'>, <type 'NoneType'>, <type 'list'>]),\n",
            " 'elevation': set([<type 'float'>, <type 'NoneType'>, <type 'list'>]),\n",
            " 'governmentType_label': set([<type 'NoneType'>, <type 'str'>, <type 'list'>]),\n",
            " 'homepage': set([<type 'NoneType'>, <type 'str'>, <type 'list'>]),\n",
            " 'isPartOf_label': set([<type 'NoneType'>, <type 'str'>, <type 'list'>]),\n",
            " 'maximumElevation': set([<type 'float'>, <type 'NoneType'>, <type 'list'>]),\n",
            " 'minimumElevation': set([<type 'float'>, <type 'NoneType'>]),\n",
            " 'name': set([<type 'NoneType'>, <type 'str'>, <type 'list'>]),\n",
            " 'populationDensity': set([<type 'float'>, <type 'NoneType'>, <type 'list'>]),\n",
            " 'populationTotal': set([<type 'int'>, <type 'NoneType'>, <type 'list'>]),\n",
            " 'timeZone_label': set([<type 'NoneType'>, <type 'str'>, <type 'list'>]),\n",
            " 'utcOffset': set([<type 'int'>,\n",
            "                   <type 'float'>,\n",
            "                   <type 'NoneType'>,\n",
            "                   <type 'str'>,\n",
            "                   <type 'list'>]),\n",
            " 'wgs84_pos#lat': set([<type 'float'>, <type 'NoneType'>, <type 'list'>]),\n",
            " 'wgs84_pos#long': set([<type 'float'>, <type 'NoneType'>, <type 'list'>])}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFSigFcvrrrP",
        "colab_type": "text"
      },
      "source": [
        "# Crossfield Auditing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0bPAJCqrrrR",
        "colab_type": "text"
      },
      "source": [
        "In this problem set you work with cities infobox data, audit it, come up with a\n",
        "cleaning idea and then clean it up.\n",
        "\n",
        "If you look at the full city data, you will notice that there are couple of\n",
        "values that seem to provide the same information in different formats: \"point\"\n",
        "seems to be the combination of \"wgs84_pos#lat\" and \"wgs84_pos#long\". However,\n",
        "we do not know if that is the case and should check if they are equivalent.\n",
        "\n",
        "Finish the function check_loc(). It will recieve 3 strings: first, the combined\n",
        "value of \"point\" followed by the separate \"wgs84_pos#\" values. You have to\n",
        "extract the lat and long values from the \"point\" argument and compare them to\n",
        "the \"wgs84_pos# values, returning True or False.\n",
        "\n",
        "Note that you do not have to fix the values, only determine if they are\n",
        "consistent. To fix them in this case you would need more information. Feel free\n",
        "to discuss possible strategies for fixing this on the discussion forum.\n",
        "\n",
        "Once you are done editig the code of the check_loc function, call the function process_file and examine the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wKUwyJxBrrrT",
        "colab_type": "code",
        "outputId": "936b6c8f-bf5d-4af1-9647-6af251bfc5e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import csv\n",
        "import pprint\n",
        "\n",
        "CITIES = 'cities.csv'\n",
        "\n",
        "\n",
        "def check_loc(point, lat, longi):\n",
        "    # YOUR CODE HERE\n",
        "    point_list = point.split(\" \")\n",
        "    if point_list[0]==lat and point_list[1]==longi:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "    pass\n",
        "\n",
        "\n",
        "def process_file(filename):\n",
        "    data = []\n",
        "    with open(filename, \"r\") as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        #skipping the extra matadata\n",
        "        for i in range(3):\n",
        "            l = reader.next()\n",
        "        # processing file\n",
        "        for line in reader:\n",
        "            # calling your function to check the location\n",
        "            result = check_loc(line[\"point\"], line[\"wgs84_pos#lat\"], line[\"wgs84_pos#long\"])\n",
        "            if not result:\n",
        "                print \"{}: {} != {} {}\".format(line[\"name\"], line[\"point\"], line[\"wgs84_pos#lat\"], line[\"wgs84_pos#long\"])\n",
        "            data.append(line)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def test():\n",
        "    assert check_loc(\"33.08 75.28\", \"33.08\", \"75.28\") == True\n",
        "    assert check_loc(\"44.57833333333333 -91.21833333333333\", \"44.5783\", \"-91.2183\") == False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    test()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6svj-f1rrrc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}